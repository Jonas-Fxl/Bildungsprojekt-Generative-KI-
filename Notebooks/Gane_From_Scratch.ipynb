{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T17:50:16.315769Z",
     "start_time": "2024-06-20T17:50:15.727623Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sonstige Bibliotheken",
   "id": "643654d99d9de7c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:50:18.847949Z",
     "start_time": "2024-06-20T17:50:16.317237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "id": "dc8747bb3cb366a0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:50:18.944184Z",
     "start_time": "2024-06-20T17:50:18.848631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../Data/Shakespeare_data.csv')\n",
    "training_data = data['PlayerLine'].str.lower()"
   ],
   "id": "1ed8e70011760980",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "5ab9f4175b004965"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:50:20.339233Z",
     "start_time": "2024-06-20T17:50:18.945415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Tokenizer erstellen und fitten\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['PlayerLine'])\n",
    "\n",
    "#Wörter in Integer umwandeln\n",
    "sequences = tokenizer.texts_to_sequences(data['PlayerLine'])\n",
    "\n",
    "#Maximale länge bestimmen (Wichtig für die Layer!)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "#Mithilfe von 0 alle sequenzen auf dieselbe länge bringen\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(sequences[:3])"
   ],
   "id": "de4317576e233f66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 315    3    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 135    3  802    1  571    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  51   88  338   45  397    5 1199    1  879    5 2367   47 3079 1629\n",
      "     2  509    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:50:20.350797Z",
     "start_time": "2024-06-20T17:50:20.339908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_words_sum = len(tokenizer.word_index)+1\n",
    "print(total_words_sum)\n",
    "\n",
    "max_seq_len = max(len(x) for x in sequences)\n",
    "print(max_seq_len)"
   ],
   "id": "5cac19bebaa92c25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25576\n",
      "167\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "total_words_sum = len(tokenizer.word_index)+1\n",
    "print(total_words_sum)\n",
    "\n",
    "max_seq_len = max(len(x) for x in sequences)\n",
    "print(max_seq_len)\n",
    "\n",
    "25576\n",
    "167"
   ],
   "id": "29ea105dc0b09c36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generator class",
   "id": "9d8c5c1a1f8f1a5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GAN:\n",
    "    def __init__(self):\n",
    "        next()"
   ],
   "id": "24659815f24f268a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "578cdde4b1049409"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
